{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2 over DNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to detect C2 channels over DNS?\n",
    "\n",
    "Using DNS as a mean of communication for a C2 channel has a high chance of success because DNS is one of few protocols to be (quite) always allowed in outbound connections.  \n",
    "\n",
    "Hunters can investigate C2 communications over DNS by collecting queries made by internal endpoints for external resources over a period of 12/24 hours and watch for high-entropy FQDNs and domains with a huge amount of subdomains.  \n",
    "\n",
    "**But, why is that?**\n",
    "\n",
    "A C2 over DNS communication channel relies on agents beaconing for instructions at a fixed time interval (attackers may inject jitter to mimic users activity).\n",
    "The requests are encoded or encrypted as subdomains like answers from the C2 server are.\n",
    "\n",
    "The sum of all subdomains per domain over a 12/24 hours packet capture just sticks out because very few companies in the world have hundreds of subdomains (Microsoft, Amazon, Google, Akamai) and none has more than 1000.  \n",
    "Also, encoding and ecnryption produces high-entropy strings (amount of randomness in a string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's import some data\n",
    "\n",
    "This Jupyter Notebook takes Zeek DNS logs and parse them to extract useful information.  \n",
    "If you only happen to have a PCAP file you can install a Zeek container from [ActiveCountermeasures' GitHub](https://github.com/activecm/docker-zeek/) and use the command `zeek readpcap <absolute path of the source file> <absolute path of the destination folder>` to create a list of logs out of it.  \n",
    "\n",
    "From the destination folder upload the file `dns.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328b4ac3dbce495db130767403641382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.log', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Upload a dns.log file\n",
    "button = widgets.FileUpload(accept=\".log\", multiple=False)\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "All right, now it is time to parse the log file and extract what we need: the DNS queries.  \n",
    "First of all, the content of the uploaded file is a [memoryview](https://docs.python.org/3/library/stdtypes.html#memory-views) so, in order to access its content, we need to decode the stream of bytes and get the *text*.\n",
    "\n",
    "### We are not done yet\n",
    "Since strings are strings, there is a little more work to do before we can actually do something.  \n",
    "What we have now is just a wall of text but we need to split at *newlines* character (`\\n`), which gives us an array or list of *rows*.  \n",
    "\n",
    "After that we need to split each row at *tab* characters (`\\t`) so to have a list of values per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get content of the file\n",
    "import codecs\n",
    "log: str = codecs.decode(button.value[0].content, encoding=\"utf-8\")\n",
    "# Split lines at newline character\n",
    "rows: list = log.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ! Shannon Entropy !\n",
    "\n",
    "Hold on! Before we continue there is one last but important thing to do... define a function to calculate the [Shannon Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)).  \n",
    "As stated earlier, strings with random characters produces a high level of entropy which we can leverage to filter out legit traffic from our analysis and spot what needs more attention faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def shannon_entropy(data):\n",
    "    if not data:\n",
    "        return 0\n",
    "    entropy = 0\n",
    "    length = len(data)\n",
    "    counts = Counter(data)\n",
    "    for count in counts.values():\n",
    "        p_x = count / length\n",
    "        entropy += -p_x * math.log(p_x, 2)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safelisting\n",
    "\n",
    "Remember to safelist legit domains to remove them from the final output.  \n",
    "Safelisting is useful to reduce the noise from non-malicious traffic and should be performed each time a new log file is analyzed.  \n",
    "\n",
    "It takes each time less work to exclude legit domains while it speeds up the analysis because the safelist already contains a lot of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries: dict = {}\n",
    "# List of domain names to ignore\n",
    "safelist = [\"_tcp.local\", \"_udp.local\", \"windowsupdate.com\", \"msedge.net\", \"googlevideo.com\", \"youtube.com\", \"microsoft.com\", \"ubuntu.com\", \"msn.com\"]\n",
    "for row in rows:\n",
    "    # Ignore first lines of Zeek logs which starts with a # symbol\n",
    "    if not row.startswith(\"#\"):\n",
    "        splitted_line: list = row.split(\"\\t\")\n",
    "        \n",
    "        # Skip unknown entries, too short to be DNS entries \n",
    "        if len(splitted_line) < 24:\n",
    "            continue\n",
    "        \n",
    "        # Extract queries and domain names\n",
    "        query: str = splitted_line[9]\n",
    "        \n",
    "        domain: str = \".\".join(query.split(\".\")[-2:])\n",
    "        \n",
    "        # Skip safelisted domains and queries\n",
    "        if domain in safelist or query in safelist:\n",
    "            continue\n",
    "        \n",
    "        domain_entropy: float = shannon_entropy(domain)\n",
    "        \n",
    "        subdomain: str = query.replace(domain, \"\")[:-1]\n",
    "        if domain not in queries.keys():\n",
    "            queries[domain] = {}\n",
    "            queries[domain][\"count\"] = 0\n",
    "            queries[domain][\"subdomains\"] = {}\n",
    "            \n",
    "        if subdomain not in queries[domain][\"subdomains\"].keys():\n",
    "            queries[domain][\"subdomains\"][subdomain] = {}\n",
    "            queries[domain][\"subdomains\"][subdomain][\"count\"] = 0\n",
    "            queries[domain][\"subdomains\"][subdomain][\"entropy\"] = shannon_entropy(subdomain)\n",
    "\n",
    "        queries[domain][\"count\"] += 1\n",
    "        queries[domain][\"subdomains\"][subdomain][\"count\"] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Now that the cells above have done their job, we have a dictionary with the following structure:\n",
    "\n",
    "```\n",
    "Domain: dict\n",
    "| - \"count\": int\n",
    "| - \"subdomains\": dict\n",
    "| - - subdomain: dict\n",
    "| - - - \"count\": int\n",
    "| - - - \"entropy\": float\n",
    "```\n",
    "\n",
    "Each key is a domain name whose keys are **count** and **subdomains**.  \n",
    "While the former is an integer counter, the latter is a dictionary whose keys are subdomains to the domain.  \n",
    "Each subdomain have two keys, **count** and **entropy** which is the value of the Shannon Entropy of the subdomain.  \n",
    "\n",
    "Knowing the dictionary layout, we can parse it and produce data for [pandas](https://pandas.pydata.org/docs/index.html).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domains ordered by unique subdomains counter\n",
    "\n",
    "The purpose of this DataFrame is to determine which domains have the highest number of unique subdomains.  \n",
    "Domains having hundreds, or even thousands, of subdomains need further investigations because (if not well-known like Google, Akamai, Microsoft, Amazon...) may be related to a C2 channel over DNS.  \n",
    "\n",
    "#### DataFrame layout\n",
    "\n",
    "|Domain|Unique Subdomains|Times looked up|\n",
    "|-|-|-|\n",
    "|Domain name|Total number of unique subdomains|Total number of queries in which the domain has been found|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data: list = [{\"Domain\": x[0], \"Unique Subdomains\": len(x[1][\"subdomains\"].keys()), \"Times looked up\": x[1]['count']} for x in queries.items()]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"Unique Subdomains\", \"Times looked up\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subdomains ordered by entropy score\n",
    "\n",
    "The purpose of this DataFrame is to determine what subdomains have the highest entropy score.  \n",
    "Entropy is useful to determine the amount of randomness in a string which may be indication of encoding or encryption.\n",
    "\n",
    "\n",
    "#### DataFrame layout\n",
    "\n",
    "|Domain|Subdomain|Times looked up|Entropy|\n",
    "|-|-|-|-|\n",
    "|Domain name|Subdomain name|Total number of queries in which the domain has been found|Entropy score|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomain_data: list = []\n",
    "for domain in queries.items():\n",
    "    for subdomain in domain[1][\"subdomains\"].items():\n",
    "        subdomain_data.append({\"Domain\": domain[0],\n",
    "                               \"Subdomain\": subdomain[0],\n",
    "                               \"Times looked up\": subdomain[1]['count'],\n",
    "                               \"Entropy\": subdomain[1]['entropy']})\n",
    "        \n",
    "df_subdomains = pd.DataFrame(subdomain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_rows = 100\n",
    "#pd.options.display.max_colwidth = 200\n",
    "df_subdomains.sort_values(by=[\"Entropy\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found something?\n",
    "\n",
    "- Are there domains with hundreds or thousands of subdomains? => **Investigate further**\n",
    "- Are there subdomains with high entropy score (4.0+)? => **Investigate further**\n",
    "- Are there domains with few subdomains but have a *strange/uncommon* name? => **Listen to your intuition and investigate further**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blueteam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
